---
title: "Classification problem"
output: html_notebook
---

> Conceptual

# 1

Let $Z=e^{\beta_0+\beta_1X}$,

Equation (4.2) becomes

  __Step 1:__ $p(X) = \frac{Z}{1+Z}$
 
  __Step 2:__ $\frac{1}{p(X)} = \frac{1+Z}{Z} = 1+\frac{1}{Z}$
 
  __Step 3:__ $Z = \frac{1}{\frac{1}{p(X)}-1} = \frac{1}{\frac{1-p(X)}{p(X)}} = \frac{p(X)}{1-p(X)}$


# 2


Equation (4.12): $p_k(x) = \frac {\pi_k \frac {1} {\sqrt{2 \pi} \sigma} \exp(- \frac {1} {2 \sigma^2} (x - \mu_k)^2) } {\sum { \pi_l \frac {1} {\sqrt{2 \pi} \sigma} \exp(- \frac {1} {2 \sigma^2} (x - \mu_l)^2) }}$

Substitute $C = \frac { \frac {1} {\sqrt{2 \pi} \sigma} \exp(- \frac {1} {2 \sigma^2} (x^2)) } {\sum { \pi_l \frac {1} {\sqrt{2 \pi} \sigma} \exp(- \frac {1} {2 \sigma^2} (x - \mu_l)^2) }}$ as this term does not vary across $k$

  __Step 1:__ Equation becomes $p_k(x) = C \pi_k \exp(- \frac {1} {2 \sigma^2} (\mu_k^2 - 2x \mu_k))$
  
  __Step 2:__ Take log of both sides $log(p_k(x)) = log(C) + log(\pi_k) + (- \frac {1} {2 \sigma^2} (\mu_k^2 - 2x \mu_k))$
  
  __Step 3:__ Simplify and rearrange $log(p_k(x)) =  (\frac {2x \mu_k} {2 \sigma^2} -\frac {\mu_k^2} {2 \sigma^2}) + log(\pi_k) + log(C)$


> Applied

# 10

## Part a)
```{r}
library(ISLR)
names(Weekly)
```

```{r}
dim(Weekly)
summary(Weekly)
```

```{r}
cor(Weekly[,-9]) # Direction is a qualitatice variable

```

```{r}
attach(Weekly) 
plot(Volume)
plot(Volume, Direction)
```

## Part b)


```{r}

glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=Weekly,family=binomial)

summary(glm.fit)
```

```{r}
library(car)
vif(glm.fit)
```

Lag 2 appears to be significant.

## Part c)


```{r}


```

Given these predictions, the `table()` function can be used to produce a __confusion matrix__ in order to determine how many observations were correctly or incorrectly classified.

```{r}
glm.pred=rep("Down",1089) # creates a vector of 1250 Down elements

# Transforms to up, all of the elements for which the predicted prob of a 
# market increase exceeds 0.5
glm.probs=predict(glm.fit,type="response") 
glm.pred[glm.probs >.5]="Up" 

table(glm.pred, Direction)

mean(glm.pred==Direction )

557/(557+430)
```

## Part d)
Logistic Regression
```{r}
max(Year)
```

```{r}
train=(Year<2009)
Weekly.2010 = Weekly[!train,] #selects all data after 2005 (data not in the trianing set)
Weekly.train = Weekly[train,]
dim(Weekly.2010)

train.Direction = Direction[train]
Direction.2010=Direction[!train]
```


```{r}
glm.fit=glm(Direction~Lag2, data=Weekly.train, family=binomial, subset=train)
glm.probs=predict(glm.fit,Weekly.2010,type="response")

glm.pred=rep("Down",104) # creates a vector of 1250 Down elements

# Transforms to up, all of the elements for which the predicted prob of a 
# market increase exceeds 0.5
glm.probs=predict(glm.fit,Weekly.2010, type="response") 
glm.pred[glm.probs >.5]="Up" 

table(glm.pred, Direction.2010)

mean(glm.pred==Direction.2010 )

32/(32+17)
```

## Part e)
LDA

```{r}
library(MASS)
lda.fit = lda(Direction~Lag2, data=Weekly, subset=train)
lda.fit
plot(lda.fit)

```

```{r}
lda.pred=predict(lda.fit, Weekly.2010) 

lda.class=lda.pred$class
table(lda.class, Direction.2010)
mean(lda.class==Direction.2010)
56/(56+34)
```

## Part f)
QDA

```{r}
qda.fit = qda(Direction~Lag2, data=Weekly, subset=train)
qda.fit
```

```{r}
qda.pred=predict(qda.fit, Weekly.2010) 

qda.class=qda.pred$class
table(qda.class, Direction.2010)
mean(qda.class==Direction.2010)
```


## Part g)

repeat with k=1

```{r}
train.yrs <- Weekly$Year %in% (1990:2008)
train <- Weekly[train.yrs,]
test <- Weekly[!train.yrs,]

require(class)
set.seed(1)
train.X <- as.matrix(Weekly.train$Lag2)
test.X <- as.matrix(Weekly.2010$Lag2)
knn.pred <- knn(train.X, test.X, train$Direction, k=1)
table(knn.pred, test$Direction)
mean(knn.pred == test$Direction)  # Accuracy=0.500
```

# Part h

Logistic Regression and LDA models performed the best

```{r, warning=FALSE, message=FALSE}
knn.pred <- knn(train.X, test.X, train$Direction, k=5)
table(knn.pred, test$Direction)
mean(knn.pred == test$Direction)
knn.pred <- knn(train.X, test.X, train$Direction, k=10)
table(knn.pred, test$Direction)
mean(knn.pred == test$Direction)
knn.pred <- knn(train.X, test.X, train$Direction, k=20)
table(knn.pred, test$Direction)
mean(knn.pred == test$Direction)
knn.pred <- knn(train.X, test.X, train$Direction, k=30)
table(knn.pred, test$Direction)
mean(knn.pred == test$Direction)
```

Higher k values for KNN (around 20) seemed to produce the best results when using only Lag2 as predictor.

```{r, warning=FALSE, message=FALSE}
fit.lda <- lda(Direction~Lag2+I(Lag1^2), data=train)
fit.lda.pred <- predict(fit.lda, test)$class
table(fit.lda.pred, test$Direction)
mean(fit.lda.pred == test$Direction)  # Accuracy=0.644
```

# Exercise 11

The goal here is to predict whether a car will have high or low gas mileage

```{r}
attach(Auto)
summary(Auto)
```

## Part a)

Creating a new variable, whether the car has high (if its mileage > median) or low mileage 

```{r}
mpg01 = ifelse(Auto$mpg > median(Auto$mpg), 1, 0)
df <- data.frame(Auto, mpg01)
head(df)
```

## Part b)

```{r}
pairs(df)

```

Acceleration, weight, horsepower, and maybe displacement look like they may be good indeicators

## Part c)

Train test split of 70/30
```{r}
set.seed(1)
trainid = sample(1:nrow(df), nrow(df)*0.7 , replace=F)  # 70% train, 30% test
train = df[trainid,]
test = df[-trainid,]
```

## Part d)

predict mpg01 with LDA

```{r}
lda.fit = lda(mpg01~displacement+horsepower+weight+acceleration, data=train)
lda.fit
plot(lda.fit)

lda.pred <- predict(lda.fit, test)$class
table(lda.pred, test$mpg01)
mean(lda.pred != test$mpg01)  # error rate
mean(lda.pred == test$mpg01)  # accuracy
```

## Part e)

predict mpg01 with QDA

```{r}
qda.fit = qda(mpg01~displacement+horsepower+weight+acceleration, data=train)
qda.fit

qda.pred = predict(qda.fit, test)$class
table(qda.pred, test$mpg01)
mean(qda.pred != test$mpg01)  # error rate
mean(qda.pred == test$mpg01)  # accuracy
```

## Part e)

predict mpg01 with logistic regression

```{r}
log.fit = glm(mpg01~displacement+horsepower+weight+acceleration, data=train, family=binomial)
log.fit
summary(log.fit)

log.prob <- predict(log.fit, test, type="response")
log.pred <- ifelse(log.prob > 0.5, 1, 0)

table(log.pred, test$mpg01)
mean(log.pred != test$mpg01)  # error rate
mean(log.pred == test$mpg01)  # accuracy
```

## Part g)

KNN

```{r, warning=FALSE, message=FALSE}
train.X <- cbind(train$displacement, train$horsepower, train$weight, train$acceleration)
test.X <- cbind(test$displacement, test$horsepower, test$weight, test$acceleration)
knn.pred <- knn(train.X, test.X, train$mpg01, k=1)
table(knn.pred, test$mpg01)
mean(knn.pred != test$mpg01)
knn.pred <- knn(train.X, test.X, train$mpg01, k=10)
table(knn.pred, test$mpg01)
mean(knn.pred != test$mpg01)
knn.pred <- knn(train.X, test.X, train$mpg01, k=20)
table(knn.pred, test$mpg01)
mean(knn.pred != test$mpg01)
knn.pred <- knn(train.X, test.X, train$mpg01, k=30)
table(knn.pred, test$mpg01)
mean(knn.pred != test$mpg01)
knn.pred <- knn(train.X, test.X, train$mpg01, k=50)
table(knn.pred, test$mpg01)
mean(knn.pred != test$mpg01)
knn.pred <- knn(train.X, test.X, train$mpg01, k=100)
table(knn.pred, test$mpg01)
mean(knn.pred != test$mpg01)
knn.pred <- knn(train.X, test.X, train$mpg01, k=200)
table(knn.pred, test$mpg01)
mean(knn.pred != test$mpg01)
```

k=1 k=20 and k=30 all have low test error rates

# Exercise 12

## Part a)
```{r}
Power = function(){
  print(2^3)
}
Power()
```
## Part b)
```{r}

Power2 = function(x,a){
  print(x^a)
}
Power2(2,3)
Power2(3,8)
```
## Part c)

```{r}
Power2(10,3)
Power2(8,17)
Power2(131,3)
```

## Part d)

```{r}
Power3 <- function(x, a) {
  return(x^a)
}
Power3(3,8)
```

__Part e)__

```{r}
x = 1:10
plot(x, Power3(x,2), xlab='x',ylab='x^2', main="x^2 vs. x")

plot(x, Power3(x,2), log="y", main="log(x^2) vs. x",
     xlab="x", ylab="log(x^2)")

plot(x, Power3(x,2), log="x",
     xlab="x", ylab="x^2")

plot(x, Power3(x,2), log="xy")

```

## Part f)

```{r}
PlotPower <- function(x, a) {
  plot(x, Power3(x,2), main="x^a versus x",
       xlab="x", ylab=paste0("x^",a))
}
PlotPower(1:10,3)
```

# Exercise 13

Use the Boston Dataset to predict whether a given suburb has a crime rate above or below the median

```{r}
dim(Boston)
names(Boston)
```

```{r}
attach(Boston)
hist(crim)
median(crim)
```

```{r}
crim01 = ifelse(Boston$crim > median(Boston$crim), 1, 0)

df = data.frame(Boston, crim01)
head(df) 
```
```{r}
summary(df)
```

```{r}
pairs(df)
```

zn, indus, nox, rm, agee, dis, rad, rax, ptratio, black, lstat, medv look interesting

### Train test split
```{r}
set.seed(1)
trainid = sample(1:nrow(df), nrow(df)*0.7 , replace=F)  # 70% train, 30% test
train = df[trainid,]
test = df[-trainid,]
```




```{r}
log.fit = glm(crim01~.-crim-chas-rm, data=train, family=binomial)
summary(log.fit)

log.prob = predict(log.fit, test, type="response")
log.pred = ifelse(log.prob > 0.5, 1, 0)

table(log.pred, test$crim01)
mean(log.pred != test$crim01)  # error rate
mean(log.pred == test$crim01)  # accuracy


```

```{r}
lda.fit = lda(crim01~.-crim-chas-rm, data=train)
lda.fit
plot(lda.fit)

lda.pred = predict(lda.fit, test)$class
table(lda.pred, test$crim01)
mean(lda.pred != test$crim01)  # error rate
mean(lda.pred == test$crim01)  # accuracy
```

```{r}
qda.fit = qda(crim01~.-crim-chas-rm, data=train)
qda.fit

qda.pred = predict(qda.fit, test)$class
table(qda.pred, test$crim01)
mean(qda.pred != test$crim01)  # error rate
mean(qda.pred == test$crim01)  # accuracy
```

QDA outperformed lda, suggesting there is a non-linear nature to this dataset

KNN

```{r, warning=FALSE, message=FALSE}
train.X <- cbind(train$zn, train$indus, train$nox, train$age, train$dis, train$rad, train$tax, train$ptratio, train$black,
                 train$lstat, train$medv)
test.X <- cbind(test$zn, test$indus, test$nox, test$age, test$dis, test$rad, 
                test$tax, test$ptratio, test$black, test$lstat, test$medv)

knn.pred <- knn(train.X, test.X, train$crim01, k=1)
table(knn.pred, test$crim01)
mean(knn.pred != test$crim01)
knn.pred <- knn(train.X, test.X, train$crim01, k=10)
table(knn.pred, test$crim01)
mean(knn.pred != test$crim01)
knn.pred <- knn(train.X, test.X, train$crim01, k=20)
table(knn.pred, test$crim01)
mean(knn.pred != test$crim01)
knn.pred <- knn(train.X, test.X, train$crim01, k=2)
table(knn.pred, test$crim01)
mean(knn.pred != test$crim01)
knn.pred <- knn(train.X, test.X, train$crim01, k=3)
table(knn.pred, test$crim01)
mean(knn.pred != test$crim01)
knn.pred <- knn(train.X, test.X, train$crim01, k=4)
table(knn.pred, test$crim01)
mean(knn.pred != test$crim01)
knn.pred <- knn(train.X, test.X, train$crim01, k=5)
table(knn.pred, test$crim01)
mean(knn.pred != test$crim01)
```

k=1 performs very well on the test set.

Overall KNN performs the best followed by QDA. This suggests there is room for improvement in logistic regression by adding higher power terms


